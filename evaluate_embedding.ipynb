{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Apr  2 17:13:01 2020\n",
    "\n",
    "@author: mor\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "import subprocess\n",
    "import unicodedata\n",
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec,FastText\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pickle\n",
    "import time\n",
    "import multiprocessing\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.cluster import KMeans\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "BASE = os.getcwd()\n",
    "DATA = os.path.join(BASE,'data')\n",
    "MODELS = os.path.join(BASE,'models')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Evaluate word2vec, fastText and doc2vec\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# word2vec, fastText evaluate\n",
    "# =============================================================================\n",
    "\n",
    "# load models\n",
    "modelWord2Vec =  Word2Vec.load(MODELS+\"/word2vec.model\")\n",
    "modelFastText =  FastText.load(MODELS+\"/fasttext.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# model vectors\n",
    "# =============================================================================\n",
    "modelWord2Vec_hb=modelWord2Vec.wv\n",
    "modelFastText_hb=modelFastText.wv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# model corpus\n",
    "# =============================================================================\n",
    "words_Word2Vec=[]\n",
    "for word in modelWord2Vec_hb.vocab:\n",
    "    words_Word2Vec.append(word)\n",
    "\n",
    "words_FastText=[]\n",
    "for word in modelFastText_hb.vocab:\n",
    "    words_FastText.append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  \n",
    "# Printing out number of tokens available\n",
    "print(\"Number of Tokens word2vec: {}\".format(len(words_Word2Vec)))\n",
    "print(\"Number of Tokens word2vec: {}\".format(len(words_FastText)))\n",
    "\n",
    "# Printing out the dimension of a word vector \n",
    "print(\"Dimension of a word vector: {}\".format(len(modelWord2Vec_hb[words_Word2Vec[0]])))\n",
    "print(\"Dimension of a word vector: {}\".format(len(modelFastText_hb[words_FastText[0]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick a word \n",
    "#word1, word2, word3 .....\n",
    "find_similar_to = 'word1'\n",
    "\n",
    "# Finding out similar words [default= top 10]\n",
    "top=20\n",
    "for similar_word in modelWord2Vec_hb.similar_by_word(find_similar_to,top):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for similar_word in modelFastText_hb.similar_by_word(find_similar_to,top):\n",
    "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
    "        similar_word[0], similar_word[1]\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# similarity    \n",
    "# =============================================================================\n",
    "sim=modelFastText_hb.similarity('word1', 'word2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelFastText_hb\n",
    "words = words_FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "#import random\n",
    "\n",
    "#random.seed(100)\n",
    "# Limit number of tokens to be visualized\n",
    "limit = 600\n",
    "vector_dim = 300\n",
    "\n",
    "embedding = []\n",
    "#randword=random.sample(words, limit)\n",
    "\n",
    "# Appending the vectors \n",
    "for word in words[:limit]:\n",
    "    embedding.append(model[word])\n",
    "\n",
    "\n",
    "# Reshaping the embedding vector\n",
    "embedding=np.array(embedding)\n",
    "#embedding = embedding.reshape(limit, vector_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#plot tsne\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png',reverse=False):\n",
    "    assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "    plt.figure(figsize=(18, 18))  # in inches\n",
    "    \n",
    "    reverse_text = -1 if reverse else 1\n",
    "    for i, label in enumerate(labels):\n",
    "        x, y = low_dim_embs[i, :]\n",
    "        plt.scatter(x, y)\n",
    "        plt.annotate(label[::reverse_text],##reverse text\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "#    plt.savefig(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embedding = tsne.fit_transform(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embedding, words[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# word embedding for documents/cases\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\S+')\n",
    "\n",
    "df = pd.read_csv('data/cases.csv')\n",
    "df[\"tokens\"] = df[\"text\"].apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# average embedding\n",
    "# =============================================================================\n",
    "def get_average_word2vec(tokens_list, vector, generate_missing=False, k=100):\n",
    "    if len(tokens_list)<1:\n",
    "        return np.zeros(k)\n",
    "    if generate_missing:\n",
    "        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in tokens_list]\n",
    "    else:\n",
    "        vectorized = [vector[word] if word in vector else np.zeros(k) for word in tokens_list]\n",
    "    length = len(vectorized)\n",
    "    summed = np.sum(vectorized, axis=0)\n",
    "    averaged = np.divide(summed, length)\n",
    "    return averaged\n",
    "\n",
    "def get_word2vec_embeddings(vectors, tokenDF, generate_missing=False):\n",
    "    embeddings = tokenDF.apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))\n",
    "    return list(embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# calculations embedding for cases\n",
    "# =============================================================================\n",
    "\n",
    "embeddingAVG = get_word2vec_embeddings(model, df[\"tokens\"])\n",
    "embeddingAVG = np.array(embeddingAVG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "caselabels = list(df.case)\n",
    "limit=500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Creating the tsne plot [Warning: will take time]\n",
    "tsne = TSNE(perplexity=30.0, n_components=2, init='pca', n_iter=5000)\n",
    "\n",
    "low_dim_embeddingAVG = tsne.fit_transform(embeddingAVG[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Finally plotting and saving the fig \n",
    "plot_with_labels(low_dim_embeddingAVG, caselabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "ks = range(1, 50)\n",
    "inertias = []\n",
    "\n",
    "for k in ks:\n",
    "    # Create a KMeans instance with k clusters: model\n",
    "    modelKMeans=KMeans(n_clusters=k)\n",
    "    \n",
    "    # Fit model to samples\n",
    "    modelKMeans.fit(embeddingAVG[:limit])\n",
    "    \n",
    "    # Append the inertia to the list of inertias\n",
    "    inertias.append(modelKMeans.inertia_)\n",
    "plt.figure(figsize=(12, 12))    \n",
    "# Plot ks vs inertias\n",
    "plt.plot(ks, inertias, '-o')\n",
    "plt.xlabel('number of clusters, k')\n",
    "plt.ylabel('inertia')\n",
    "plt.xticks(ks)\n",
    "#plt.savefig('kmeans_inertia.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import KMeans\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Create a KMeans instance with 3 clusters: model\n",
    "modelKMeans = KMeans(n_clusters=22)\n",
    "\n",
    "# Fit model to points\n",
    "modelKMeans.fit(embeddingAVG[:limit])\n",
    "\n",
    "labels = modelKMeans.predict(embeddingAVG[:limit])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assign the columns of new_points: xs and ys\n",
    "xs = low_dim_embeddingAVG[:,0]\n",
    "ys = low_dim_embeddingAVG[:,1]\n",
    "plt.figure(figsize=(24, 16))\n",
    "# Make a scatter plot of xs and ys, using labels to define the colors\n",
    "plt.scatter(xs,ys,c=labels,alpha=0.5)\n",
    "for x, y, number in zip(xs, ys, range(0,len(xs))):\n",
    "    plt.annotate(number, (x, y), xytext=(5, 5),textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "# Assign the cluster centers: centroids\n",
    "centroids = modelKMeans.cluster_centers_\n",
    "\n",
    "# Assign the columns of centroids: centroids_x, centroids_y\n",
    "centroids_x = centroids[:,0]\n",
    "centroids_y = centroids[:,1]\n",
    "\n",
    "# Make a scatter plot of centroids_x and centroids_y\n",
    "plt.scatter(centroids_x,centroids_y,marker='D',s=50,color='red')\n",
    "plt.savefig('kmeansDoc.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# simmilarity\n",
    "# =============================================================================\n",
    "\n",
    "indices = np.random.permutation(np.arange(len(embeddingAVG)))[:limit]\n",
    "dfemg=pd.DataFrame(embeddingAVG[indices],index=indices).sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simmilarity=dfemg.T.corr()\n",
    "print(simmilarity[9].nsmallest(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(low_dim_embeddingAVG).sort_index().T.corr()[9].nlargest(20) #.nsmallest(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(embeddingAVG[[214,477]]).sort_index().T.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# =============================================================================\n",
    "# heat map for similarity\n",
    "# =============================================================================\n",
    "\n",
    "def heatMap(df, mirror, filename,annot=False):\n",
    "\n",
    "    # Create Correlation df\n",
    "    corr = df#.corr()\n",
    "    # Plot figsize\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    # Generate Color Map\n",
    "    colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "   \n",
    "    if mirror == True:\n",
    "       #Generate Heat Map, allow annotations and place floats in map\n",
    "       sns.heatmap(corr, cmap=colormap, annot=annot, fmt=\".2f\")\n",
    "       #Apply xticks\n",
    "#       plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "       #Apply yticks\n",
    "#       plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "       #show plot\n",
    "\n",
    "    else:\n",
    "       # Drop self-correlations\n",
    "       dropSelf = np.zeros_like(corr)\n",
    "       dropSelf[np.triu_indices_from(dropSelf)] = True# Generate Color Map\n",
    "       colormap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "       # Generate Heat Map, allow annotations and place floats in map\n",
    "       sns.heatmap(corr, cmap=colormap, fmt=\".2f\",annot=annot, mask=dropSelf)\n",
    "       # Apply xticks\n",
    "#       plt.xticks(range(len(corr.columns)), corr.columns);\n",
    "       # Apply yticks\n",
    "#       plt.yticks(range(len(corr.columns)), corr.columns)\n",
    "    # show plot\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatMap(simmilarity,False,filename='heatmap2dim.png') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Keyword Similarity - represent the relevancy of documnet by vetor of similarity values\n",
    "# =============================================================================\n",
    "\n",
    "#list of key words\n",
    "litools=['kword1','kword2','kword3']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenssim(s,col):\n",
    "    listtokens=list(s['newtokens'])\n",
    "    sim=0\n",
    "    simmax=0\n",
    "    wordmax=''\n",
    "    for i in listtokens:\n",
    "        sim=modelFastText_hb.similarity(i, col)\n",
    "        if sim>simmax:\n",
    "            simmax=sim\n",
    "            wordmax=i\n",
    "    s[col+'0']=wordmax\n",
    "    s[col+'1']=simmax\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for c in litools:\n",
    "    df=df.apply(lambda row: tokenssim(row,col=c), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# doc2vec evaluate\n",
    "# =============================================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "\n",
    "#load model\n",
    "model = Doc2Vec.load(MODELS+\"/doc2vec.model\")\n",
    "\n",
    "df = pd.read_csv(DATA+'/cases.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#to find the vector of a document which is not in training data\n",
    "test_data = word_tokenize(\"word1 word22 word12\")\n",
    "v1 = model.infer_vector(test_data)\n",
    "print(\"V1_infer\", v1)\n",
    "\n",
    "# to find most similar doc using tags\n",
    "similar_doc = model.docvecs.most_similar('id')\n",
    "print(similar_doc)\n",
    "\n",
    "\n",
    "# to find vector of doc in training data using tags(id)\n",
    "print(model.docvecs['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "vects=[]\n",
    "columns=[]\n",
    "for i,row in df.iterrows():\n",
    "    vects.append(model.docvecs[str(row['id'])])\n",
    "    columns.append(row['id'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vectsnp=np.array(vects)\n",
    "columnsnp=np.array(columns)\n",
    "dfvec=pd.DataFrame(vectsnp,index=columnsnp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# similarity\n",
    "# =============================================================================\n",
    "\n",
    "sim=cosine_similarity(dfvec)\n",
    "cases=dfvec.index\n",
    "simdf=pd.DataFrame(sim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "heatMap(simdf,False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatMap(simdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatMap(simdf.iloc[70:80,38:40],mirror=True,annot=True,filename='heatMapdocZoom.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "heatMap(simdf.iloc[70:80,0:10],mirror=True,annot=True,filename='heatMapdocZoom2.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# check text cases\n",
    "# =============================================================================\n",
    "\n",
    "case1=cases[73]\n",
    "case2=cases[39]\n",
    "\n",
    "textdf=df[np.logical_or(df.case==case1,df.case==case2)]\n",
    "for idx , row in textdf.iterrows():\n",
    "    print(row['text'])\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "case1=cases[73]\n",
    "case2=cases[3]\n",
    "\n",
    "textdf=df[np.logical_or(df.case==case1,df.case==case2)]\n",
    "for idx , row in textdf.iterrows():\n",
    "    print(row['text'])\n",
    "    print(\"\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# find similar docs\n",
    "# =============================================================================\n",
    "\n",
    "#document len\n",
    "textdf['ln']=textdf.text.str.len()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "threshold = 0.6\n",
    "\n",
    "docslist=[]\n",
    "similar_docs=set()\n",
    "for i,row in textdf.iterrows():\n",
    "    similar_doc,val = model.docvecs.most_similar(row['id'],topn=1)[0]\n",
    "    if val > threshold:\n",
    "        similar_docs.add(similar_doc)\n",
    "        if row['id'] in similar_docs:\n",
    "            continue\n",
    "        print(similar_doc,val)\n",
    "        docslist.append([row['id'],similar_doc,val,row['ln']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docdf=pd.DataFrame(docslist,columns=['doc_a','doc_b','sim','ln']).sort_values('sim',ascending=False)\n",
    "\n",
    "print(docdf.head(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docdf[np.logical_and(docdf['sim'].between(0.7,1),docdf['ln']>400)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "docdf[docdf['sim'].between(0.7,0.91)]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
