{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Apr  3 20:01:22 2020\n",
    "\n",
    "@author: mor\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "import codecs\n",
    "import string\n",
    "import subprocess\n",
    "import unicodedata\n",
    "import os\n",
    "from io import StringIO\n",
    "import pandas as pd\n",
    "from operator import itemgetter\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Data preprocessing , docx format\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Set important directory\n",
    "BASE = os.getcwd()\n",
    "DOCS = os.path.join(BASE, \"data\")\n",
    "## Create a path to extract the corpus.\n",
    "CORPUS = os.path.join(BASE, \"data\", \"corpus\")\n",
    "MODELS = os.path.join(BASE,'models')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Returns a filtered list of paths to .docx files representing our corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_documentsDOC(path=DOCS):\n",
    "    for name in os.listdir(path):\n",
    "        if name.endswith('.docx'):\n",
    "            yield os.path.join(path, name)\n",
    "\n",
    "\n",
    "def get_documentsDOC2(path=DOCS):\n",
    "    for file_path in glob.glob(DOCS+'\\\\*.docx'):\n",
    "        yield file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "# Print the total number of documents\n",
    "print(len(list(get_documentsDOC())))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path='..\\\\data\\\\8b3f32dc3f214f1f9ba0bfa717703900.docx'\n",
    "path2='..\\\\data\\\\1a016400391c4cd4a1926407ad02fe1a.docx'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Module that extract text from MS XML Word document (.docx).\n",
    "(Inspired by python-docx <https://github.com/mikemaccana/python-docx>)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from xml.etree.cElementTree import XML\n",
    "except ImportError:\n",
    "    from xml.etree.ElementTree import XML\n",
    "import zipfile\n",
    "from io import BytesIO, StringIO               \n",
    "from docx import Document   \n",
    "\n",
    "\n",
    "WORD_NAMESPACE = '{http://schemas.openxmlformats.org/wordprocessingml/2006/main}'\n",
    "PARA = WORD_NAMESPACE + 'p'\n",
    "TEXT = WORD_NAMESPACE + 't'\n",
    "\n",
    "\n",
    "\n",
    "#Take the path of a docx file, return the body text\n",
    "def get_docx_text(path):\n",
    "\n",
    "    document = zipfile.ZipFile(path)\n",
    "    xml_content = document.read('word/document.xml')\n",
    "    document.close()\n",
    "    tree = XML(xml_content)\n",
    "\n",
    "    paragraphs = []\n",
    "    for paragraph in tree.getiterator(PARA):\n",
    "        texts = [node.text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "                 for node in paragraph.getiterator(TEXT)\n",
    "                 if node.text]\n",
    "        if texts:\n",
    "            texts=standardize_text(texts)\n",
    "            paragraphs.append(''.join(texts))\n",
    "            paragraphs=standardize_text(paragraphs)\n",
    "    return '\\n'.join(paragraphs)\n",
    "\n",
    "#Take the path of a docx file, return the header text\n",
    "def get_headers(path):\n",
    "    document = zipfile.ZipFile(path)\n",
    "    headers = []\n",
    "    for name in document.namelist():\n",
    "        if 'header' in name:\n",
    "            xml_content = document.read(name)    \n",
    "            tree = XML(xml_content)\n",
    "\n",
    "            \n",
    "            for paragraph in tree.getiterator(PARA):\n",
    "                texts = [node.text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "                         for node in paragraph.getiterator(TEXT)\n",
    "                         if node.text]\n",
    "                if texts:\n",
    "                    texts=standardize_text(texts)\n",
    "                    headers.append(' '.join(texts))\n",
    "                    headers=standardize_text(headers)\n",
    "    document.close()\n",
    "    return '\\n'.join(headers)\n",
    "\n",
    "\n",
    "def standardize_text(texts):\n",
    "    newTexts=[]\n",
    "    for t in texts:\n",
    "#        t=t.strip('\\s')\n",
    "        t = re.sub(r\"[^A-Za-z0-9(),!?@\\':\\`\\-\\.\\\"\\_\\na-z\\u0590-\\u05fe]\", ' ', t)\n",
    "        t = t.lower()\n",
    "        t=re.sub(r\"[0-9]+[\\-:\\s][0-9]+([\\-:\\s][0-9]+)?\\s\",\"\",t)\n",
    "        t=  t.encode('utf-8', 'ignore').decode('utf-8')\n",
    "        t=  re.sub('\\s+',' ',t)\n",
    "        newTexts.append(t)\n",
    "    return newTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('header')\n",
    "print(get_headers(path2))\n",
    "print('\\ntext')\n",
    "print(get_docx_text(path2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Create txt file for every docx file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Extracts a text corpus from the docx documents and writes them to disk as a .txt.\n",
    "def extract_corpus(docs=DOCS, corpus=CORPUS):\n",
    "#    i=0\n",
    "    # Create corpus directory if it doesn't exist.\n",
    "    if not os.path.exists(corpus):\n",
    "        os.mkdir(corpus)\n",
    "    \n",
    "    for path in get_documentsDOC(docs):\n",
    "        print(path)\n",
    "        document = get_headers(path)\n",
    "        document = document + (get_docx_text(path))\n",
    "        document=document.replace('\\n',' ')\n",
    "        document=standardize_text([document])[0]\n",
    "        \n",
    "        fname = os.path.splitext(os.path.basename(path))[0] + \".txt\"\n",
    "        outpath = os.path.join(corpus, fname)\n",
    "        with codecs.open(outpath, 'w', encoding='utf-8', errors='replace') as f: #, encoding='utf-8', errors='replace'\n",
    "            f.write(document)\n",
    "    print('******************** preprocess docx files   -  end *******************')\n",
    "    print('******************** created txt files -  end *******************')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "          \n",
    "# Run the extraction of text\n",
    "extract_corpus()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "###  Data preprocessing txt file\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def readFiles(path):\n",
    "    data=[]\n",
    "    filesNames=[]\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            with codecs.open(os.path.join(path,filename), 'r',encoding=\"utf-8\") as content_file:\n",
    "                file_content = content_file.read()\n",
    "    #            print(str(file_handle.name))\n",
    "    #            print(file_content)\n",
    "                filesNames.append(str(filename))\n",
    "                data.append(file_content)\n",
    "    \n",
    "    return filesNames,data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "filesnames,articles=readFiles(CORPUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Files to pandas dataframe\n",
    "# =============================================================================\n",
    "\n",
    "basic = pd.DataFrame(list(zip(filesnames,articles)),columns=['filename','text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def standardize_textDF(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z,!?@\\'\\.\\\"\\_\\na-z\\u0590-\\u05fe]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r'\\s+',\" \")\n",
    "    df[text_field] = df[text_field].str.strip()\n",
    "    return df[text_field]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic['textclean']=standardize_textDF(basic,'text')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\S+')\n",
    "\n",
    "basic[\"tokens\"] = basic[\"text\"].apply(tokenizer.tokenize)\n",
    "basic[\"tokensclean\"] = basic[\"textclean\"].apply(tokenizer.tokenize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "def removeStartAndEndPunctuation (tokens_list):\n",
    "    tokens_list = [re.sub(r\"^[^A-Za-za-z\\u0590-\\u05fe]+\", \"\",word) for word in tokens_list]\n",
    "    tokens_list = [re.sub(r\"[^A-Za-za-z\\u0590-\\u05fe]+$\", \"\",word) for word in tokens_list]\n",
    "    tokens_list = [word for word in tokens_list if word != '']\n",
    "    tokens_list = [word for word in tokens_list if len(word) > 1]\n",
    "    return tokens_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic['tokenNoPunctuation']=basic['tokensclean'].apply(lambda x: removeStartAndEndPunctuation(x))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
